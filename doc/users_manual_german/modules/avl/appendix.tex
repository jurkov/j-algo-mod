\newtheorem{alg}{Algorithmus}

\chapter{\label{appendix_avl_A}Einleitung zu Datenstrukturen}
\begin{tabbing}
\textbf{Anmerkung:} \=Der Autor der folgenden Seiten (Kapitel \ref{appendix_avl_A}, \ref{appendix_avl_B} und \ref{appendix_avl_C} dieses Anhangs) ist\\
\> Jean Christoph Jung (Teammitglied AVL-Modul)
\end{tabbing}

Eine häufige Anwendung auf großen Datenmengen ist das Suchen: Das Wiederfinden eines bestimmten Elements oder bestimmter Informationen aus einer großen Menge früher abgelegter Daten. Um die Suche zu vereinfachen, werden den (möglicherweise sehr komplexen) großen Datensätzen eineindeutige Suchschlüssel (Keys) zugeordnet. Der Vergleich zweier solcher Schlüssel ist in der Regel viel schneller als der Vergleich zweier Datensätze. Wegen der Schlüsselzuordnung reicht es aus, alle vorkommenden Algorithmen nur auf den Schlüsseln zu betrachten; in Wirklichkeit verweisen erst die Schlüssel auf die Datensätze.\\
Eine grundlegende Idee ist nun, die Daten in Form einer Liste oder eines Feldes abzulegen. Diese Datenstruktur ist höchst einfach. Jedoch ist der Aufwand für das Suchen relativ hoch, nämlich $O(n)$. Genauer gesagt benötigt man für eine erfolglose Suche $n$ Vergleiche (bei $n$ Elementen in der Datenstruktur), denn man muss jedes Element überprüfen, und für eine erfolgreiche Suche durchschnittlich $(n+1)/2$ Vergleiche durchführen, da nach jedem Element mit der gleichen Wahrscheinlichkeit gesucht wird.\\
Eine Verbesserung dieses Verfahrens wäre, die Daten sortiert abzulegen, was zu einer binären Suche führt. Die Suche hat jetzt nur noch Komplexität $O(\log_2{n})$, da bei jedem Suchschritt das Feld halbiert wird. Der Nachteil ist, dass durch die Sortierung das Einfügen erschwert wird, da unter Umständen viele Datensätze bewegt werden müssen. Das Verfahren sollte also nur angewandt werden, wenn sehr wenige oder gar keine Einfügeoperationen ausgeführt werden müssen. Dann können die Daten anfangs mit einem schnellen Verfahren sortiert werden und müssen danach nicht mehr verändert werden.\\
Eine weitere Datenstruktur, die der Suchbäume, wollen wir hier vorstellen.

\chapter{\label{appendix_avl_B}Suchbäume}
Suchbäume sind binäre Bäume (jeder Knoten hat höchstens 2 Kinder) mit der Eigenschaft:\\
Für jeden Knoten gilt: alle Schlüssel im rechten Teilbaum sind größer als der eigene Schlüssel und alle Schlüssel im linken Teilbaum sind kleiner. Diese Eigenschaft wird hier immer Suchbaumeigenschaft genannt.

\centerpic{avl/bsp_suchbaum}{4.5}{Ein Beispiel für einen Suchbaum} 

An dieser Stelle noch eine Bemerkung zu den Schlüsseln: Die Schlüssel können Elemente einer beliebigen Menge sein, unter der Bedingung, dass auf dieser Menge eine Ordnungsrelation definiert ist. Die Ordnungsrelation wird offensichtlich für die Suchbaumeigenschaft benötigt, da dort die Begriffe "`kleiner"' und "`größer"' vorkommen. Eine häufig verwendete Menge sind die natürlichen Zahlen mit ihrer normalen Ordnungsrelation $\leq$. Alle Operationen auf Suchbäumen kann man sich also anhand der natürlichen Zahlen vorstellen.\\
Aus der Suchbaumeigenschaft kann man sich leicht rekursive Algorithmen für das Einfügen und Suchen in einem Suchbaum herleiten:

\begin{alg} \label{search}
	(Suchen eines Schlüssels $s$ in einem Suchbaum)
	\begin{enumerate}
		\item Falls Teilbaum leer, dann Schlüssel nicht im Baum vorhanden
		\item Falls $s$ gleich dem Schlüssel des aktuellen Knoten, dann Suche erfolgreich.
		\item Falls $s$ größer als Schlüssel des aktuellen Knotens, dann suche (rekursiv) $s$ im rechten Teilbaum.
		\item Falls $s$ kleiner als Schlüssel des aktuellen Knotens, dann suche (rekursiv) $s$ im linken Teilbaum.
	\end {enumerate}
\end{alg}

\newpage
\begin{alg} \label{insert}
	(Einfügen eines Schlüssels $s$ in einen Suchbaum)
	\begin {enumerate}
		\item Falls Teilbaum leer, dann neuen Schlüssel hier einfügen.
		\item Falls $s$ gleich dem Schlüssel des aktuellen Knotens, dann Schlüssel bereits vorhanden, Einfügen nicht nötig.
		\item Falls $s$ größer als Schlüssel des aktuellen Knotens, dann füge (rekursiv) $s$ in den rechten Teilbaum ein.
		\item Falls $s$ kleiner als Schlüssel des aktuellen Knotens, dann füge (rekursiv) $s$ in den linken Teilbaum ein.
	\end {enumerate}
\end{alg}

Mit Algorithmus \ref{insert} ergibt sich folgende Eigenschaft der Struktur von Suchbäumen: im Gegensatz zur sortierten Liste hängt die Struktur eines Baumes davon ab, in welcher Reihenfolge die Elemente eingefügt werden. So erhält man verschiedene Bäume, wenn man $1, 2, 3, 4$ in dieser Reihenfolge und in der Reihenfolge $3, 2, 4, 1$ einfügt.

\centerpic{avl/12345}{4}{Unterschiedliche Suchbäume bei unterschiedlicher Einfügereihenfolge} 

Im ersten Fall erhält man einen Suchbaum, der zur linearen Liste entartet ist. Das ist nicht nur in dieser speziellen Reihenfolge so, es gibt viele Möglichkeiten einen entarteten Baum zu erzeugen. Der Algorithmus hat also zwei Nachteile: zum einen kann der Suchaufwand linear zur Anzahl der Knoten im Baum sein, was keine Verbesserung zur linearen Liste darstellt; zum anderen hängt die Güte des Verfahrens von der Eingabefolge ab. Der Vorteil von Suchbäumen wird klar, wenn man einen "`vollen"' Baum betrachtet, d.h. alle Pfade zu Blättern haben dieselbe Länge. Dann hat sowohl das Suchen (unabhängig davon, ob der Schlüssel im Baum vorhanden ist) als auch das Einfügen eine Komplexität von $O(\log{n})$.

\centerpic{avl/vollerbaum}{5}{Ein Beispiel für einen vollen Baum}
\medskip
Es gibt einige Algorithmen, bei denen der Einfügealgorithmus so modifiziert ist, dass die entarteten Fälle vermieden werden und immer nahezu volle Bäume entstehen. Einen davon, den Algorithmus nach Adelson-Velskij und Landis (AVL), werden wir später betrachten.

Doch zunächst wollen wir noch eine weitere wichtige Operation auf Suchbäumen untersuchen: das Löschen. Leider ist es nicht ganz so einfach wie Suche und Einfügen.\\
Zuerst muss der zu löschende Schlüssel gesucht werden. Ist der betreffende Knoten ein Blatt, kann er einfach entfernt werden. Hat der zu löschende Knoten nur ein Kind, kann er durch dieses ersetzt werden. Der schwierige Fall ist, wenn er zwei Kinder hat. Damit die Suchbaum\-eigenschaft erhalten bleibt, muss man ihn durch den nächst größeren Schlüssel ersetzen. Der nächst größere Schlüssel befindet sich offensichtlich im rechten Teilbaum.

\centerpic{avl/remove}{5}{Löschen eines Knoten im Suchbaum}
\medskip
Nach dem Ersetzen bleibt die Suchbaumeigenschaft erhalten, weil alle Schlüssel aus dem linken Teilbaum ohnehin kleiner sind als die aus dem rechten. Außerdem sind auch alle Schlüssel aus dem rechten Teilbaum größer, sonst wäre es nicht der nächst größere Schlüssel gewesen. Aus diesen Überlegungen erhält man folgende verbale Beschreibung des Löschen-Algorithmus:

\newpage
\begin{alg} \label{delete}
	(Löschen eines Schlüssels $s$ aus einem Suchbaum)
	\begin{enumerate}
		\item Suche s nach Algorithmus \ref{search}. Falls s nicht im Baum enthalten, terminiert der Algorithmus.
		\item \begin{enumerate}
			\item Ist der zu löschende Knoten ein Blatt, dann entferne ihn einfach aus dem Baum.
			\item Hat der zu löschende Knoten nur ein Kind, dann ersetze ihn durch dieses.
			\item Sonst suche den kleinsten Schlüssel im rechten Teilbaum: Gehe zum rechten Kind und dann immer zum linken Teilbaum, solange dieser nicht leer ist. Ersetze den zu löschenden Schlüssel durch den des so gefundenen Knotens. Ersetze den gefundenen Knoten durch sein rechtes Kind.
			\end{enumerate}
	\end{enumerate}
\end{alg}

Man kann anstelle des nächst größeren Schlüssels genausogut den nächstkleineren nehmen, der Algorithmus funktioniert trotzdem. Zur Komplexität ist zu sagen, dass der Algorithmus maximal $h$ Vergleiche macht, wobei $h$ die Höhe des Baumes ist. Auch hier ist also die Komplexität abhängig von der Struktur des Baumes.

\chapter{\label{appendix_avl_C}AVL-Bäume}
AVL-Bäume (benannt nach Adelson-Velskij und Landis) sind spezielle Suchbäume: In jedem Knoten unterscheiden sich die Höhen des linken Teilbaums und des rechten Teilbaums um höchstens 1. Um diese Eigenschaft (AVL-Eigenschaft) abzusichern, wird für jeden Knoten ein Balancefaktor eingeführt. Der Balancefaktor ist die Differenz der Höhen des rechten Teilbaums und des linken Teilbaums. Also gilt für jeden AVL-Baum, dass alle Balancefaktoren aus $\{-1,0,1\}$ sind. Bäume mit AVL-Eigenschaft sind niemals als lineare Liste entartet (sofern sie denn mehr als 2 Knoten haben), sondern sind immer fast vollständig. Zwischen der Höhe $h$ eines Baumes und der Anzahl $n$ seiner Knoten besteht folgender Zusammenhang: $h\leq 2 \cdot \log_2{n}$. Das bedeutet, dass für das Suchen logarithmische Komplexität garantiert werden kann (das Suchen erfolgt gemäß Algorithmus \ref{search}).

\centerpic{avl/avlbsp}{5}{Ein Beispiel für einen AVL-Baum mit Balancen}
\medskip
Jetzt muss noch untersucht werden, wie groß der zusätzliche Aufwand beim Einfügen ist, um die AVL-Eigenschaft zu wahren. In jedem Fall wird der neue Knoten als Blatt eingefügt (nach demselben Algorithmus wie bei Suchbäumen). Dabei kann sich der Balancefaktor ändern. Allerdings kann man sich leicht klarmachen, dass das nur entlang des Suchpfades passieren kann. Die Aktualisierung von Balancefaktoren erfolgt von der Einfügestelle zur Wurzel. Hier der Algorithmus:

\newpage
\begin{alg} \label{avlinsert}
	(Algorithmus zum Einfügen eines Elements x in einen AVL-Baum)
	\begin{enumerate}
		\item Füge das neue Element x als direkten Nachfolger des Knotens n als Blatt ein, sodass die Suchbaumeigenschaft erfüllt bleibt. Aktualisiere n.balance.
		\item Setze n auf den Vorgängerknoten von n.
			\begin{enumerate}
				\item Falls x im linken Unterbaum von n eingefügt wurde
					\begin{enumerate}
						\item wenn n.balance==1 dann n.balance=0 und gehe nach 3.
						\item wenn n.balance==0, dann n.balance=-1 und gehe nach 2.
						\item wenn n.balance==-1 und 
							\begin{itemize}
								\item wenn n.left.balance==-1, dann Rechts(n)-Rotation.
								\item wenn n.left.balance==1 dann Links(n.left)-Rechts(n)-Rotation.
							\end{itemize}
							Gehe zu 3.
					\end{enumerate}
				\item Falls x im rechten Unterbaum von n eingefügt wurde
					\begin{enumerate}
						\item wenn n.balance==-1 dann n.balance=0 und gehe nach 3.
						\item wenn n.balance==0, dann n.balance=1 und gehe nach 2.
						\item wenn n.balance==1 und
							\begin{itemize}
								\item wenn n.left.balance==1, dann Links(n)-Rotation.
								\item wenn n.left.balance==-1 dann Rechts(n.left)-Links(n)-Rotation.
							\end{itemize}
							Gehe zu 3.
					\end{enumerate}
			\end{enumerate}
		\item Gehe zurück zur Wurzel.
	\end{enumerate}
\end{alg}

Zur Analyse dieses Algorithmus: Das reine Einfügen erfolgt gemäß Einfügen im Suchbaum (Algorithmus \ref{insert}), allerdings ist hier sichergestellt, dass sich die Höhe logarithmisch zur Anzahl der Knoten verhält, d.h. auch der Aufwand für das Einfügen ist garantiert logarithmisch. Wie gesagt können sich jedoch Balancefaktoren geändert haben, sodass die AVL-Eigenschaft nicht mehr erfüllt ist. Das wird durch sogenannte Rotationen behoben. Es gibt zwei Typen von Rotationen -- Linksrotation und Rechtsrotation, jeweils um einen Knoten n.

\centerpic{avl/rotateleft}{5}{Linksrotation um den Knoten 65}  \medskip
\centerpic{avl/rotateright}{5}{Rechtsrotation um den Knoten 58}  \medskip

Falls durch das Einfügen irgendwo ein Balancefaktor $2$($-2$) entsteht (größere Änderungen können beim Einfügen eines Knotens offensichtlich nicht auftreten), heißt das, dass sich im rechten (linken) Teilbaum die Höhe um 1 erhöht hat. Durch Rotation(en) wie im Algorithmus angegeben, wird aber genau diese Höhe wieder reduziert. Somit ist klar, dass man maximal zweimal rotieren muss, der Aufwand ist also noch erträglich, im Gegensatz zum Löschen, wie man gleich sehen wird.

Genauso wie Einfügen verändert auch Löschen eines Knotens aus einem AVL-Baum die Balancefaktoren, also müssen auch hier Rotationen ausgeführt werden. Hier der Algorithmus:

\begin{alg} \label{avldelete}
	Löschen eines Knotens aus einem AVL-Baum)
	\begin{enumerate}
		\item Lösche den Knoten analog zum Löschen im Suchbaum (Algorithmus \ref{delete}). Falls der Knoten ein Blatt war oder nur einen linken Nachbarn hatte, setze aktuellen Knoten auf den Vater. Sonst setze aktuellen Knoten auf den Vater des Knotens mit dem nächst größeren Schlüssel.
		\item Berechne den Balancefaktor des aktuellen Knotens neu. Falls
			\begin{enumerate}
				\item Balance 2 und rechte Balance -1, dann Rechts(n.right)-Links(n)-Rotation.
				\item Balance 2 und rechte Balance nicht -1, dann Links(n)-Rotation.
				\item Balance -2 und linke Balance 1, dann Links(n.left)-Rechts(n)-Rotation.
				\item Balance -2 und linke Balance nicht 1, dann Rechts(n)-Rotation.
				\item sonst keine Rotation.
			\end{enumerate}
			Wiederhole diesen Schritt solange, bis die Wurzel erreicht ist.
	\end{enumerate}
\end{alg}

Auch hier wollen wir den Aufwand des Algorithmus etwas genauer untersuchen. Schritt 1 entspricht dem Löschen aus dem Suchbaum, nur garantiert mit logarithmischen Aufwand. Die Frage ist nun, ob, wie beim Einfügen, der Algorithmus mit maximal zwei Rotationen auskommt. Leider ist das nicht der Fall. Das liegt an der erwähnten Eigenschaft der Rotationen, sie verringern die Höhe eines Teilbaums. Beim Einfügen war das gut, da durch das Anhängen eines Knotens gerade die Höhe vergrößert wurde. Hier jedoch ist das unvorteilhaft, es kann passieren, dass man mehrere Rotationen auf dem Weg zur Wurzel durchführen muss. Die Anzahl der Rotationen ist nur durch die Höhe des Baums beschränkt. Das ist in Anwendungsfällen nicht wünschenswert.

Bei zeitkritischen Anwendungen muss man also entweder auf einen anderen Algorithmus ausweichen, oder Varianten wie etwa Lazy-Delete implementieren, d.h. der Knoten wird nur als gelöscht markiert und wird später (wenn Zeit ist) aus dem Baum entfernt.

\begin{thebibliography}{99}
    \bibitem {sedgewick} R. Sedgewick, "`Algorithmen in C++"', Addison-Wesley, 5. Auflage, 1999
    \bibitem {vogler} Prof. Vogler, "`Vorlesungsskript Algorithmen, Datenstrukturen und Programmierung"', TU Dresden, 2003
    \bibitem {wiki} http://de.wikipedia.org/wiki/AVL-Baum
    \bibitem {uni-leipzig} http://dbs.uni-leipzig.de/de/skripte/ADS1/HTML/kap6-11.html
\end{thebibliography}